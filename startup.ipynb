{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\prave\\anaconda3\\lib\\site-packages (0.2.16)\n",
      "Requirement already satisfied: transformers in c:\\users\\prave\\anaconda3\\lib\\site-packages (4.44.2)\n",
      "Requirement already satisfied: neo4j in c:\\users\\prave\\anaconda3\\lib\\site-packages (5.24.0)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\prave\\anaconda3\\lib\\site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: pinecone-client in c:\\users\\prave\\anaconda3\\lib\\site-packages (5.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement Neo4jGraph (from versions: none)\n",
      "ERROR: No matching distribution found for Neo4jGraph\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain transformers neo4j faiss-cpu pinecone-client neo4j Neo4jGraph pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dBurl = \"neo4j+s://b26b71fa.databases.neo4j.io\"\n",
    "password = \"YWpXBbSXHIcloDAhR_ZfTo9iR6jLzFIwyux_bNBU6lA\"\n",
    "username = \"neo4j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.graphs import Neo4jGraph\n",
    "from neo4j import GraphDatabase\n",
    "import requests\n",
    "\n",
    "graph = Neo4jGraph(dBurl, username, password)\n",
    "driver = GraphDatabase.driver(dBurl, auth=(username, password))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prave\\anaconda3\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\prave\\AppData\\Local\\Temp\\ipykernel_10952\\3111894771.py:7: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "c:\\Users\\prave\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load embeddings\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "documents = [\"This is a test document\", \"This is another test document\"]\n",
    "faiss_store = FAISS.from_texts(documents, embedder)\n",
    "\n",
    "# from langchain.vectorstores.neo4j_vector import Neo4jVector\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# hf_embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# vector_index = Neo4jVector.from_existing_graph(\n",
    "#     hf_embeddings,\n",
    "#     url = dBurl,\n",
    "#     username = username,\n",
    "#     password = password,\n",
    "#     index_name='tasks',\n",
    "#     node_label=\"Tasks\",\n",
    "#     text_node_properties=['name', 'description', 'status'],\n",
    "#     embedding_node_property=\"embedding\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='This is another test document'), Document(page_content='This is a test document')]\n"
     ]
    }
   ],
   "source": [
    "def run_query(query):\n",
    "    with driver.session() as session:\n",
    "        result = session.run(query)\n",
    "        return [record.data() for record in result]\n",
    "\n",
    "def hybrid_retrieval(query):\n",
    "    # First try retrieving from the knowledge graph\n",
    "    kg_result = run_query(f\"MATCH (n) WHERE n.name CONTAINS '{query}' RETURN n LIMIT 1\")\n",
    "    \n",
    "    if kg_result:\n",
    "        return kg_result[0]  # If found in KG\n",
    "    \n",
    "    # If not, fall back to vector search\n",
    "    docs = faiss_store.similarity_search(query)\n",
    "    return docs[0].page_content if docs else None\n",
    "\n",
    "responses = faiss_store.similarity_search(\"How will RecommendationService be updated?\")\n",
    "print(responses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load a language model\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "# Function to generate a response using retrieved context\n",
    "def generate_response(query):\n",
    "    context = hybrid_retrieval(query)\n",
    "    prompt = f\"Answer the question based on the following context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "    \n",
    "    response = generator(prompt, max_length=100, num_return_sequences=1)\n",
    "    return response[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Welcome to the Knowledge Graph-based RAG Chatbot!\")\n",
    "query = \"How many open tickets there are?\"\n",
    "response = generate_response(query)\n",
    "print(f\"Bot: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
